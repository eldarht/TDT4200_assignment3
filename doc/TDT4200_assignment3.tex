\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\title{TDT4200 assignment 3}

\begin{document}
\maketitle
\section*{Introduction}
This document aims to answer the tasks given by the third assignment in TDT4200.
The associated source code can be found at \url{https://github.com/eldarht/TDT4200_assignment3}

\section*{Task 0 And comparison}
\subsection*{3}
The time it took for the application to complete with laplacian1Kernel on average after 5 tries on my computer (Dell LATITUDE E5450) as measured by the time command:

\begin{tabular}{|l|c|c|c|}
	Itterations & Real time  & scatterv/gatherv (proc: 8) & scatterv/gatherv with ghosts (proc: 8)\\
	\hline
	1			& 0.253s	 & 0.743s	& 0.738s\\
	1024		& 1m 38.798s & 50.176s	& 54.662s\\
\end{tabular}

The cpu is a Intel(R) Core(TM) i5-5300U CPU @ 2.30GHz with 4 cores and two threads per core(Hyper threaded).

\section*{Task 2}
\subsection*{b)}
The amount of communication varies depending on the number of ranks. For this task I assume 8 ranks, as that is a good fit for a four core CPU with hyper threading. \\

\subsubsection*{Before the loop (setup)}
\begin{tabular}{l|p{200px}|c|}
	call & description & data \\
	\hline
	MPI\_Bcast & The dimensions of each image section is sendt. This will send one message to each rank, but the task of sending is delegated. & 9 unsigned int \\
	\hline
	MPI\_Scatterv & The whole image is divided and border duplicates & 5350400 unsigned char \\
\end{tabular}

\subsubsection*{The loop (Kernel convolutions)}
\begin{tabular}{l|p{200px}|c|}
	call & description & data \\
	\hline
	MPI\_Send (even) & After each loop the even numbered ranks sends one row for each ghostborder. That is one row for rank zero and two rows for rank 2, 4 and 6. With three itterations this happen three times for each ranks borders. & 53760 unsigned char \\
	\hline
	MPI\_Recv (even) & After each loop the even numbered ranks recieve one row for each of their borders. That is the same as for the even sends. & 53760 unsigned char \\	
	\hline
	MPI\_Send (odd) & After each loop the odd numbered ranks sends one row for each ghostborder. That is one row for rank 7 and two rows for rank 1, 3 and 5. This also happens 3 times for each ranks borders. & 53760 unsigned char \\
	\hline
	MPI\_Recv (odd) & After each loop the even numbered ranks recieve one row for each of their borders. That is the same as for the odd sends. & 53760 unsigned char \\
\end{tabular} \\

In this implementation, the border at the end of the section will always be sendt before the border before the section.

\subsubsection*{After the loop (Reconstruction)}
\begin{tabular}{l|p{200px}|c|}
	call & description & data \\
	\hline

	MPI\_Gatherv & The whole image reconstructed without ghostborders & 5242880 unsigned char \\
\end{tabular} \\

In Total There are 42 sends/reviecves in the loop and 45 in total, if you consider scatter, gather and broadcast as one each. The total data is 9 unsigned int and 10700800 unsigned char.

\subsection*{d)}
I wanted as much of the memory managment to be done by MPI as possible. Having the recieve write to the ghost border that was in the bmp image channel, so as to not have to use memcpy or reconstruct the image section. I did not want to tuch the $applykernel()$ function, as i knew that it already worked on the rawdata. I had read the ghost\_cell\_patterns.pdf and followed the simplest instructions for preventing deadlock. Hence the exchange/recieve order is dependent on the process rank.
\end{document}
